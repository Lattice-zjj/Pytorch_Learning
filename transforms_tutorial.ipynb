{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforms\n",
        "===================\n",
        "\n",
        "Data does not always come in its final processed form that is required for \n",
        "training machine learning algorithms. We use **transforms** to perform some\n",
        "manipulation of the data and make it suitable for training.\n",
        "\n",
        "=> (数据并不总是以训练机器学习算法所需的最终处理形式出现.我们使用**transforms**对数据进行一些操作，使其适合于训练。)\n",
        "\n",
        "All TorchVision datasets have two parameters -``transform`` to modify the features and\n",
        "``target_transform`` to modify the labels - that accept callables containing the transformation logic.\n",
        "The `torchvision.transforms <https://pytorch.org/vision/stable/transforms.html>`_ module offers \n",
        "several commonly-used transforms out of the box.\n",
        "\n",
        "=> (所有TorchVision数据集都有两个参数-``transform``->modify the features,``target_transform`` -> modify the labels;`torchvision.transforms <https://pytorch.org/vision/stable/transforms.html>`_模块提供了几种常用的现成转换。)\n",
        "\n",
        "The FashionMNIST features are in PIL Image format, and the labels are integers.\n",
        "For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\n",
        "To make these transformations, we use ``ToTensor`` and ``Lambda``.\n",
        "\n",
        "=> (FashionMNIST 是PIL图像格式,标签是整数.\n",
        "对于训练，我们需要将特征作为归一化张量,将标签作为一个one-hot encoded tensors(热编码张量).\n",
        "为了进行这些转换，我们使用了``ToTensor``和``Lambda``。)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ToTensor()\n",
        "-------------------------------\n",
        "\n",
        "`ToTensor <https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor>`_ \n",
        "converts a PIL image or NumPy ``ndarray`` into a ``FloatTensor``. and scales \n",
        "the image's pixel intensity values in the range [0., 1.]\n",
        "(将PIL图像或NumPy ``ndarray``转换为``FloatTensor``,并缩放图像的像素强度值在[0., 1.]范围内)\n",
        "\n",
        "- Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. This transform does not support torchscript.\n",
        "\n",
        "    - Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
        "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "\n",
        "    - **if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
        "    or if the numpy.ndarray has dtype = np.uint8**\n",
        "\n",
        "\n",
        "    - In the other cases, tensors are returned without scaling.\n",
        "\n",
        "    .. note::\n",
        "        Because the input image is scaled to [0.0, 1.0], this transformation should not be used when\n",
        "        transforming target image masks. See the `references`_ for implementing the transforms for image masks.\n",
        "\n",
        "    .. _references: https://github.com/pytorch/vision/tree/master/references/segmentation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lambda Transforms\n",
        "-------------------------------\n",
        "\n",
        "Lambda transforms apply any user-defined lambda function. Here, we define a function \n",
        "to turn the integer into a one-hot encoded tensor. \n",
        "It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls \n",
        "`scatter_ <https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_>`_ which assigns a \n",
        "``value=1`` on the index as given by the label ``y``.\n",
        "\n",
        "> Tensor.scatter_\n",
        "> - Writes all values from the tensor src into self at the indices specified in the index tensor.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "f = lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
        "print(f(0), f(1), f(2), sep=\"\\n\")\n",
        "target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor.scatter_(dim, index, src, reduce=None) → Tensor\n",
        "\n",
        "Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.\n",
        "\n",
        "For a 3-D tensor, self is updated as:\n",
        "```\n",
        "self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
        "self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
        "self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
        "```\n",
        "This is the reverse operation of the manner described in gather().\n",
        "\n",
        "self, index and src (if it is a Tensor) should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.\n",
        "\n",
        "Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive.\n",
        "\n",
        "WARNING\n",
        "\n",
        "When indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!\n",
        "\n",
        "NOTE\n",
        "\n",
        "The backward pass is implemented only for src.shape == index.shape.\n",
        "\n",
        "Additionally accepts an optional reduce argument that allows specification of an optional reduction operation, which is applied to all values in the tensor src into self at the indicies specified in the index. For each value in src, the reduction operation is applied to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.\n",
        "\n",
        "Given a 3-D tensor and reduction using the multiplication operation, self is updated as:\n",
        "```\n",
        "self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\n",
        "self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\n",
        "self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n",
        "```\n",
        "Reducing with the addition operation is the same as using scatter_add_().\n",
        "\n",
        "- Parameters\n",
        "    - dim (int) – the axis along which to index\n",
        "\n",
        "    - index (LongTensor) – the indices of elements to scatter, can be either empty or of the same dimensionality as src. When empty, the operation returns self unchanged.\n",
        "\n",
        "    - src (Tensor or float) – the source element(s) to scatter.\n",
        "\n",
        "    - reduce (str, optional) – reduction operation to apply, can be either 'add' or 'multiply'.\n",
        "\n",
        "Example:\n",
        "```\n",
        ">>> src = torch.arange(1, 11).reshape((2, 5))\n",
        ">>> src\n",
        "tensor([[ 1,  2,  3,  4,  5],\n",
        "        [ 6,  7,  8,  9, 10]])\n",
        ">>> index = torch.tensor([[0, 1, 2, 0]])\n",
        ">>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n",
        "tensor([[1, 0, 0, 4, 0],\n",
        "        [0, 2, 0, 0, 0],\n",
        "        [0, 0, 3, 0, 0]])\n",
        ">>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
        ">>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n",
        "tensor([[1, 2, 3, 0, 0],\n",
        "        [6, 7, 0, 0, 8],\n",
        "        [0, 0, 0, 0, 0]])\n",
        "\n",
        ">>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
        "...            1.23, reduce='multiply')\n",
        "tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
        "        [2.0000, 2.0000, 2.0000, 2.4600]])\n",
        ">>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
        "...            1.23, reduce='add')\n",
        "tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
        "        [2.0000, 2.0000, 2.0000, 3.2300]])\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further Reading\n",
        "~~~~~~~~~~~~~~~~~\n",
        "- `torchvision.transforms API <https://pytorch.org/vision/stable/transforms.html>`_\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit ('ENV': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "interpreter": {
      "hash": "a2574f2f3a7e19e373e9d857e50cdf4a305fa6b0d1e1d4dad94fd7b7b8dabd9b"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}