{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors \n",
        "==========================\n",
        "\n",
        "**Tensors是一种特殊的数据结构,与数组和矩阵非常相似.\n",
        "在PyTorch中,我们使用Tensors来编码模型的输入和输出,以及模型的参数.\n",
        "Tensors类似于 `NumPy <https://numpy.org/>` ,除了Tensors可以在gpu或其他硬件加速器上运行.**\n",
        "\n",
        "**事实上,Tensors和NumPy数组通常可以共享相同的底层内存,从而不需要复制数据(参见“bridge-to-np-label”).\n",
        "Tensors也为自动区分进行了优化(我们将在后面的`Autograd <autograd_tutorial.html>`部分).**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 初始化一个Tensor\n",
        "\n",
        "Tensors 可以通过各种方式进行初始化.看下接下来的例子：\n",
        "\n",
        "### **直接从data中初始化**\n",
        "\n",
        "Tensors可以直接从data中创建。数据类型是自动推断的。\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **从一个NumPy array中初始化**\n",
        "\n",
        "Tensors 可以从 NumPy arrays中初始化 (and vice versa - see `bridge-to-np-label`).\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **从另外一个tensor:**\n",
        "\n",
        "**新张量保留参数张量的属性(形状，数据类型)，除非显式重写。**\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "x_ones = torch.ones_like(x_data) # 保留x_data的属性\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # 覆盖x_data的数据类型\n",
        "# torch.rand_like 's function:\n",
        "#   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0, 1). \n",
        "#   torch.rand_like(input) is equivalent to torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.9123, 0.3356],\n",
            "        [0.7454, 0.5176]]) \n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **用随机值或常数值初始化:**\n",
        "\n",
        "``shape`` 是一个张量维度的元组。在下面的函数中，它决定了输出张量的维数。\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.1795, 0.1913, 0.8608],\n",
            "        [0.9474, 0.6672, 0.2368]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor 的一些属性\n",
        "\n",
        "Tensor 的属性描述了他们的shape,数据的类型,以及他们存储在哪个装置上(cpu or gpu).\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensors 上的操作\n",
        "\n",
        "> Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, \n",
        "indexing, slicing), sampling and more are\n",
        "comprehensively described `here <https://pytorch.org/docs/stable/torch.html>`__.\n",
        "\n",
        "> Each of these operations can be run on the GPU (at typically higher speeds than on a\n",
        "CPU). If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\n",
        "\n",
        "> By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using \n",
        "``.to`` method (after checking for GPU availability). Keep in mind that copying large tensors\n",
        "across devices can be expensive in terms of time and memory!\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# 如果GPU存在的话将我们的tensor转移至GPU\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**尝试列表中的一些操作.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**标准的 numpy-like 索引和切片:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print('First row: ',tensor[0])\n",
        "print('First column: ', tensor[:, 0])\n",
        "print('Last column:', tensor[..., -1])\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row:  tensor([1., 1., 1., 1.])\n",
            "First column:  tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **连接张量**\n",
        "可以使用``torch.cat``将给定维数的张量序列连接起来。\n",
        "参见`torch.stack <https://pytorch.org/docs/stable/generated/torch.stack.html>`__,另一个连接张量的操作和``torch.cat``有微妙的不同。\n",
        "\n",
        "##### **torch.cat:** \n",
        "Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
        "> torch.cat(tensors, dim=0, *, out=None) → Tensor\n",
        "- tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.(任何相同类型张量的python序列.提供的非空张量必须具有相同的形状,除了在cat维度.)\n",
        "- dim (int, optional) – the dimension over which the tensors are concatenated (张量连接的维数.)\n",
        "\n",
        "------------------\n",
        "\n",
        "##### **torch.stack:** \n",
        "Concatenates a sequence of tensors along a new dimension. All tensors need to be of the same size.\n",
        "> torch.stack(tensors, dim=0, *, out=None) → Tensor\n",
        "- tensors (sequence of Tensors) – sequence of tensors to concatenate(要连接的张量序列.)\n",
        "- dim (int) – dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive) (插入的维度.必须在0和级联张量的维数之间(包括))\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "t1_0 = torch.cat([tensor, tensor, tensor], dim=0)\n",
        "t1_1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "# t1_2 = torch.cat([tensor, tensor, tensor], dim=2) # IndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
        "print(t1_0,'\\n',t1_1)\n",
        "t2_0 = torch.stack([tensor, tensor, tensor], dim=0)\n",
        "t2_1 = torch.stack([tensor, tensor, tensor], dim=1)\n",
        "print(t2_0,'\\n',t2_1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            " tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
            "tensor([[[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]]]) \n",
            " tensor([[[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]]])\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **算术运算**\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# 这是计算两个张量之间的矩阵乘法\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(tensor)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "print(tensor, '\\n', y1,'\\n', y2,'\\n', y3,'\\n')\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "# 这是计算元素的乘积\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)\n",
        "print(tensor, '\\n', z1, '\\n', z2,'\\n', z3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            "\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **单元素张量** \n",
        "如果你有一个单元素张量，例如通过聚集所有张量\n",
        "把一个张量的值转换成一个值，可以使用 ``item()``把它转换成Python数值:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "agg = tensor.sum() # Returns the sum of all elements in the input tensor.\n",
        "agg_item = agg.item()  \n",
        "print(agg, agg_item, type(agg_item))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12.) 12.0 <class 'float'>\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **就地操作**\n",
        "Operations that store the result into the operand are called in-place. They are denoted by a ``_`` suffix. \n",
        "For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "print(tensor, \"\\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor, \"\\n\")\n",
        "random_data = torch.rand_like(tensor)\n",
        "tensor.copy_(random_data)\n",
        "print(random_data, \"\\n\", tensor)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2604, 0.2796, 0.5094, 0.0687],\n",
            "        [0.2978, 0.4229, 0.3648, 0.1990],\n",
            "        [0.5823, 0.9985, 0.1751, 0.3498],\n",
            "        [0.8418, 0.9451, 0.1670, 0.0736]]) \n",
            "\n",
            "tensor([[5.2604, 5.2796, 5.5094, 5.0687],\n",
            "        [5.2978, 5.4229, 5.3648, 5.1990],\n",
            "        [5.5823, 5.9985, 5.1751, 5.3498],\n",
            "        [5.8418, 5.9451, 5.1670, 5.0736]]) \n",
            "\n",
            "tensor([[0.5956, 0.0731, 0.7831, 0.0896],\n",
            "        [0.4536, 0.8038, 0.9504, 0.5316],\n",
            "        [0.8082, 0.9876, 0.4477, 0.3678],\n",
            "        [0.0701, 0.0245, 0.5533, 0.3587]]) \n",
            " tensor([[0.5956, 0.0731, 0.7831, 0.0896],\n",
            "        [0.4536, 0.8038, 0.9504, 0.5316],\n",
            "        [0.8082, 0.9876, 0.4477, 0.3678],\n",
            "        [0.0701, 0.0245, 0.5533, 0.3587]])\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Note**\n",
        "**就地操作可以节省一些内存,但在计算导数时可能会出现问题,因为会立即造成损失的历史.因此,不鼓励使用它们.**\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Bridge with NumPy\n",
        "> Tensors on the CPU and NumPy arrays can share their underlying memory \n",
        "locations, and changing one will change\tthe other. \n",
        "(CPU和NumPy数组上的张量可以共享它们的底层内存地点，改变一个就会改变另一个。)\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor to NumPy array\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A change in the tensor reflects in the NumPy array.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy array to Tensor\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in the NumPy array reflects in the tensor.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit ('ENV': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "interpreter": {
      "hash": "a2574f2f3a7e19e373e9d857e50cdf4a305fa6b0d1e1d4dad94fd7b7b8dabd9b"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}